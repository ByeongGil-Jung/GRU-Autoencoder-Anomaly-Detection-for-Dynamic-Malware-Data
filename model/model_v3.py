import torch
from torch import nn
from domain.metadata import Hyperparameters



class EncoderRNN(nn.Module):

    def __init__(self, hparams):
        super(EncoderRNN, self).__init__()
        self.hparams = hparams

        self.rnn_1 = nn.GRU(input_size=hparams.input_size, hidden_size=hparams.hidden_1_size, num_layers=1, batch_first=True, bias=True)
        self.rnn_2 = nn.GRU(input_size=hparams.hidden_1_size, hidden_size=hparams.hidden_2_size, num_layers=1, batch_first=True, bias=True)
        # self.rnn_3 = nn.GRU(input_size=hparams.hidden_2_size, hidden_size=hparams.hidden_3_size, num_layers=1, batch_first=True, bias=True)

    def forward(self, x):
        x, hidden_1 = self.rnn_1(x)
        x, hidden_2 = self.rnn_2(x)
        # x, hidden_3 = self.rnn_3(x)

        return x, hidden_2

class DecoderRNN(nn.Module):

    def __init__(self, hparams):
        super(DecoderRNN, self).__init__()
        self.hparams = hparams

        # self.rnn_1 = nn.GRU(input_size=hparams.hidden_3_size, hidden_size=hparams.hidden_2_size, num_layers=1, batch_first=True, bias=True)
        self.rnn_2 = nn.GRU(input_size=hparams.hidden_2_size, hidden_size=hparams.hidden_1_size, num_layers=1, batch_first=True, bias=True)
        self.rnn_3 = nn.GRU(input_size=hparams.hidden_1_size, hidden_size=hparams.input_size, num_layers=1, batch_first=True, bias=True)

    def forward(self, x):
        # x, hidden_1 = self.rnn_1(x)
        x, hidden_2 = self.rnn_2(x)
        x, hidden_3 = self.rnn_3(x)

        return x, hidden_3


class Model(nn.Module):

    def __init__(self, hparams):
        super(Model, self).__init__()
        self.hparams = hparams

        self.encoder_rnn = EncoderRNN(hparams)
        self.decoder_rnn = DecoderRNN(hparams)

    def forward(self, x):
        x, _ = self.encoder_rnn(x)
        x_hat, _ = self.decoder_rnn(x)

        return x_hat
